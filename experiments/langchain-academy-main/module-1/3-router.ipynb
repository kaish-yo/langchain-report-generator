{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce6fff79-25b5-4884-8aaa-e3ebb7ddd549",
   "metadata": {},
   "source": [
    "# Router\n",
    "\n",
    "## Review\n",
    "\n",
    "We built a graph that uses `messages` as state and a chat model with bound tools.\n",
    "\n",
    "We saw that the graph can:\n",
    "\n",
    "* Return a tool call\n",
    "* Return a natural language response\n",
    "\n",
    "## Goals\n",
    "\n",
    "We can think of this as a router, where the chat model routes between a direct response or a tool call based upon the user input.\n",
    "\n",
    "This is an simple example of an agent, where the LLM is directing the control flow either by calling a tool or just responding directly. \n",
    "\n",
    "![Screenshot 2024-08-21 at 9.24.09 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbac6543c3d4df239a4ed1_router1.png)\n",
    "\n",
    "Let's extend our graph to work with either output! \n",
    "\n",
    "For this, we can use two ideas:\n",
    "\n",
    "(1) Add a node that will call our tool.\n",
    "\n",
    "(2) Add a conditional edge that will look at the chat model model output, and route to our tool calling node or simply end if no tool call is performed. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb4fc6e-7c85-4fc8-a4a9-0c7a527c4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture --no-stderr\n",
    "# %pip install --quiet -U langchain_openai langchain_core langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "885e92d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set for tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"LangChain Academy-LangGraph\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3ba4df4-3045-49b1-9299-ced1fce14d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply a and b.\n",
    "\n",
    "    Args:\n",
    "        a: first int\n",
    "        b: second int\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "llm_with_tools = llm.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77555a2",
   "metadata": {},
   "source": [
    " We use the [built-in `ToolNode`](https://langchain-ai.github.io/langgraph/reference/prebuilt/?h=tools+condition#toolnode) and simply pass a list of our tools to initialize it. \n",
    " \n",
    " We use the [built-in `tools_condition`](https://langchain-ai.github.io/langgraph/reference/prebuilt/?h=tools+condition#tools_condition) as our conditional edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a6fde4e-cceb-4426-b770-97ee4b41e9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "# Node\n",
    "def tool_calling_llm(state: MessagesState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "builder.add_node(\"tools\", ToolNode([multiply])) # A node that runs the tools called in the last AIMessage.\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\",\n",
    "    # If the latest message (result) from assistant is a tool call -> tools_condition routes to tools\n",
    "    # If the latest message (result) from assistant is a not a tool call -> tools_condition routes to END\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", END)\n",
    "graph = builder.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "993b4793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ToolNode in module langgraph.prebuilt.tool_node:\n",
      "\n",
      "class ToolNode(langgraph.utils.runnable.RunnableCallable)\n",
      " |  ToolNode(tools: 'Sequence[Union[BaseTool, Callable]]', *, name: 'str' = 'tools', tags: 'Optional[list[str]]' = None, handle_tool_errors: 'Optional[bool]' = True) -> 'None'\n",
      " |\n",
      " |  A node that runs the tools called in the last AIMessage.\n",
      " |\n",
      " |  It can be used either in StateGraph with a \"messages\" key or in MessageGraph. If\n",
      " |  multiple tool calls are requested, they will be run in parallel. The output will be\n",
      " |  a list of ToolMessages, one for each tool call.\n",
      " |\n",
      " |  The `ToolNode` is roughly analogous to:\n",
      " |\n",
      " |  ```python\n",
      " |  tools_by_name = {tool.name: tool for tool in tools}\n",
      " |  def tool_node(state: dict):\n",
      " |      result = []\n",
      " |      for tool_call in state[\"messages\"][-1].tool_calls:\n",
      " |          tool = tools_by_name[tool_call[\"name\"]]\n",
      " |          observation = tool.invoke(tool_call[\"args\"])\n",
      " |          result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
      " |      return {\"messages\": result}\n",
      " |  ```\n",
      " |\n",
      " |  Important:\n",
      " |      - The state MUST contain a list of messages.\n",
      " |      - The last message MUST be an `AIMessage`.\n",
      " |      - The `AIMessage` MUST have `tool_calls` populated.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      ToolNode\n",
      " |      langgraph.utils.runnable.RunnableCallable\n",
      " |      langchain_core.runnables.base.Runnable\n",
      " |      typing.Generic\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, tools: 'Sequence[Union[BaseTool, Callable]]', *, name: 'str' = 'tools', tags: 'Optional[list[str]]' = None, handle_tool_errors: 'Optional[bool]' = True) -> 'None'\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __abstractmethods__ = frozenset()\n",
      " |\n",
      " |  __annotations__ = {}\n",
      " |\n",
      " |  __parameters__ = ()\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langgraph.utils.runnable.RunnableCallable:\n",
      " |\n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |\n",
      " |  async ainvoke(self, input: Any, config: Optional[langchain_core.runnables.config.RunnableConfig] = None, **kwargs: Any) -> Any\n",
      " |      Default implementation of ainvoke, calls invoke from a thread.\n",
      " |\n",
      " |      The default implementation allows usage of async code even if\n",
      " |      the Runnable did not implement a native async version of invoke.\n",
      " |\n",
      " |      Subclasses should override this method if they can run asynchronously.\n",
      " |\n",
      " |  invoke(self, input: Any, config: Optional[langchain_core.runnables.config.RunnableConfig] = None, **kwargs: Any) -> Any\n",
      " |      Transform a single input into an output. Override to implement.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: A config to use when invoking the Runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details.\n",
      " |\n",
      " |      Returns:\n",
      " |          The output of the Runnable.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this Runnable with another object to create a RunnableSequence.\n",
      " |\n",
      " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
      " |      Compose this Runnable with another object to create a RunnableSequence.\n",
      " |\n",
      " |  async abatch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs ainvoke in parallel using asyncio.gather.\n",
      " |\n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying Runnable uses an API which supports a batch mode.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the Runnable.\n",
      " |          config: A config to use when invoking the Runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Returns:\n",
      " |          A list of outputs from the Runnable.\n",
      " |\n",
      " |  async abatch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Tuple[int, Union[Output, Exception]]]'\n",
      " |      Run ainvoke in parallel on a list of inputs,\n",
      " |      yielding results as they complete.\n",
      " |\n",
      " |      Args:\n",
      " |          inputs: A list of inputs to the Runnable.\n",
      " |          config: A config to use when invoking the Runnable.\n",
      " |             The config supports standard keys like 'tags', 'metadata' for tracing\n",
      " |             purposes, 'max_concurrency' for controlling how much work to do\n",
      " |             in parallel, and other keys. Please refer to the RunnableConfig\n",
      " |             for more details. Defaults to None. Defaults to None.\n",
      " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
      " |              Defaults to False.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          A tuple of the index of the input and the output from the Runnable.\n",
      " |\n",
      " |  as_tool(self, args_schema: 'Optional[Type[BaseModel]]' = None, *, name: 'Optional[str]' = None, description: 'Optional[str]' = None, arg_types: 'Optional[Dict[str, Type]]' = None) -> 'BaseTool'\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |\n",
      " |      Create a BaseTool from a Runnable.\n",
      " |\n",
      " |      ``as_tool`` will instantiate a BaseTool with a name, description, and\n",
      " |      ``args_schema`` from a Runnable. Where possible, schemas are inferred\n",
      " |      from ``runnable.get_input_schema``. Alternatively (e.g., if the\n",
      " |      Runnable takes a dict as input and the specific dict keys are not typed),\n",
      " |      the schema can be specified directly with ``args_schema``. You can also\n",
      " |      pass ``arg_types`` to just specify the required arguments and their types.\n",
      " |\n",
      " |      Args:\n",
      " |          args_schema: The schema for the tool. Defaults to None.\n",
      " |          name: The name of the tool. Defaults to None.\n",
      " |          description: The description of the tool. Defaults to None.\n",
      " |          arg_types: A dictionary of argument names to types. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A BaseTool instance.\n",
      " |\n",
      " |      Typed dict input:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from typing import List\n",
      " |          from typing_extensions import TypedDict\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          class Args(TypedDict):\n",
      " |              a: int\n",
      " |              b: List[int]\n",
      " |\n",
      " |          def f(x: Args) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |\n",
      " |      ``dict`` input, specifying schema via ``args_schema``:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from typing import Any, Dict, List\n",
      " |          from langchain_core.pydantic_v1 import BaseModel, Field\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          def f(x: Dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |          class FSchema(BaseModel):\n",
      " |              \"\"\"Apply a function to an integer and list of integers.\"\"\"\n",
      " |\n",
      " |              a: int = Field(..., description=\"Integer\")\n",
      " |              b: List[int] = Field(..., description=\"List of ints\")\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(FSchema)\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |\n",
      " |      ``dict`` input, specifying schema via ``arg_types``:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from typing import Any, Dict, List\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          def f(x: Dict[str, Any]) -> str:\n",
      " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
      " |\n",
      " |          runnable = RunnableLambda(f)\n",
      " |          as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": List[int]})\n",
      " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
      " |\n",
      " |      String input:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          def f(x: str) -> str:\n",
      " |              return x + \"a\"\n",
      " |\n",
      " |          def g(x: str) -> str:\n",
      " |              return x + \"z\"\n",
      " |\n",
      " |          runnable = RunnableLambda(f) | g\n",
      " |          as_tool = runnable.as_tool()\n",
      " |          as_tool.invoke(\"b\")\n",
      " |\n",
      " |      .. versionadded:: 0.2.14\n",
      " |\n",
      " |  assign(self, **kwargs: 'Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Assigns new fields to the dict output of this Runnable.\n",
      " |      Returns a new Runnable.\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_community.llms.fake import FakeStreamingListLLM\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |          from langchain_core.prompts import SystemMessagePromptTemplate\n",
      " |          from langchain_core.runnables import Runnable\n",
      " |          from operator import itemgetter\n",
      " |\n",
      " |          prompt = (\n",
      " |              SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n",
      " |              + \"{question}\"\n",
      " |          )\n",
      " |          llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n",
      " |\n",
      " |          chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n",
      " |\n",
      " |          chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n",
      " |\n",
      " |          print(chain_with_assign.input_schema.schema())\n",
      " |          # {'title': 'PromptInput', 'type': 'object', 'properties':\n",
      " |          {'question': {'title': 'Question', 'type': 'string'}}}\n",
      " |          print(chain_with_assign.output_schema.schema()) #\n",
      " |          {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n",
      " |          {'str': {'title': 'Str',\n",
      " |          'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n",
      " |\n",
      " |  async astream(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of astream, which calls ainvoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |\n",
      " |  astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1', 'v2']\", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
      " |      .. beta::\n",
      " |         This API is in beta and may change in the future.\n",
      " |\n",
      " |      Generate a stream of events.\n",
      " |\n",
      " |      Use to create an iterator over StreamEvents that provide real-time information\n",
      " |      about the progress of the Runnable, including StreamEvents from intermediate\n",
      " |      results.\n",
      " |\n",
      " |      A StreamEvent is a dictionary with the following schema:\n",
      " |\n",
      " |      - ``event``: **str** - Event names are of the\n",
      " |          format: on_[runnable_type]_(start|stream|end).\n",
      " |      - ``name``: **str** - The name of the Runnable that generated the event.\n",
      " |      - ``run_id``: **str** - randomly generated ID associated with the given execution of\n",
      " |          the Runnable that emitted the event.\n",
      " |          A child Runnable that gets invoked as part of the execution of a\n",
      " |          parent Runnable is assigned its own unique ID.\n",
      " |      - ``parent_ids``: **List[str]** - The IDs of the parent runnables that\n",
      " |          generated the event. The root Runnable will have an empty list.\n",
      " |          The order of the parent IDs is from the root to the immediate parent.\n",
      " |          Only available for v2 version of the API. The v1 version of the API\n",
      " |          will return an empty list.\n",
      " |      - ``tags``: **Optional[List[str]]** - The tags of the Runnable that generated\n",
      " |          the event.\n",
      " |      - ``metadata``: **Optional[Dict[str, Any]]** - The metadata of the Runnable\n",
      " |          that generated the event.\n",
      " |      - ``data``: **Dict[str, Any]**\n",
      " |\n",
      " |\n",
      " |      Below is a table that illustrates some evens that might be emitted by various\n",
      " |      chains. Metadata fields have been omitted from the table for brevity.\n",
      " |      Chain definitions have been included after the table.\n",
      " |\n",
      " |      **ATTENTION** This reference table is for the V2 version of the schema.\n",
      " |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | event                | name             | chunk                           | input                                         | output                                          |\n",
      " |      +======================+==================+=================================+===============================================+=================================================+\n",
      " |      | on_chat_model_start  | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_stream | [model name]     | AIMessageChunk(content=\"hello\") |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chat_model_end    | [model name]     |                                 | {\"messages\": [[SystemMessage, HumanMessage]]} | AIMessageChunk(content=\"hello world\")           |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_start         | [model name]     |                                 | {'input': 'hello'}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_stream        | [model name]     | 'Hello'                         |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_llm_end           | [model name]     |                                 | 'Hello human!'                                |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_start       | format_docs      |                                 |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_stream      | format_docs      | \"hello world!, goodbye world!\"  |                                               |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_chain_end         | format_docs      |                                 | [Document(...)]                               | \"hello world!, goodbye world!\"                  |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_start        | some_tool        |                                 | {\"x\": 1, \"y\": \"2\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_tool_end          | some_tool        |                                 |                                               | {\"x\": 1, \"y\": \"2\"}                              |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_start   | [retriever name] |                                 | {\"query\": \"hello\"}                            |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_retriever_end     | [retriever name] |                                 | {\"query\": \"hello\"}                            | [Document(...), ..]                             |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_start      | [template_name]  |                                 | {\"question\": \"hello\"}                         |                                                 |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |      | on_prompt_end        | [template_name]  |                                 | {\"question\": \"hello\"}                         | ChatPromptValue(messages: [SystemMessage, ...]) |\n",
      " |      +----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+\n",
      " |\n",
      " |      In addition to the standard events, users can also dispatch custom events (see example below).\n",
      " |\n",
      " |      Custom events will be only be surfaced with in the `v2` version of the API!\n",
      " |\n",
      " |      A custom event has following format:\n",
      " |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      | Attribute | Type | Description                                                                                               |\n",
      " |      +===========+======+===========================================================================================================+\n",
      " |      | name      | str  | A user defined name for the event.                                                                        |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |      | data      | Any  | The data associated with the event. This can be anything, though we suggest making it JSON serializable.  |\n",
      " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
      " |\n",
      " |      Here are declarations associated with the standard events shown above:\n",
      " |\n",
      " |      `format_docs`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          def format_docs(docs: List[Document]) -> str:\n",
      " |              '''Format the docs.'''\n",
      " |              return \", \".join([doc.page_content for doc in docs])\n",
      " |\n",
      " |          format_docs = RunnableLambda(format_docs)\n",
      " |\n",
      " |      `some_tool`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          @tool\n",
      " |          def some_tool(x: int, y: str) -> dict:\n",
      " |              '''Some_tool.'''\n",
      " |              return {\"x\": x, \"y\": y}\n",
      " |\n",
      " |      `prompt`:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          template = ChatPromptTemplate.from_messages(\n",
      " |              [(\"system\", \"You are Cat Agent 007\"), (\"human\", \"{question}\")]\n",
      " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
      " |\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          async def reverse(s: str) -> str:\n",
      " |              return s[::-1]\n",
      " |\n",
      " |          chain = RunnableLambda(func=reverse)\n",
      " |\n",
      " |          events = [\n",
      " |              event async for event in chain.astream_events(\"hello\", version=\"v2\")\n",
      " |          ]\n",
      " |\n",
      " |          # will produce the following events (run_id, and parent_ids\n",
      " |          # has been omitted for brevity):\n",
      " |          [\n",
      " |              {\n",
      " |                  \"data\": {\"input\": \"hello\"},\n",
      " |                  \"event\": \"on_chain_start\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"chunk\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_stream\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |              {\n",
      " |                  \"data\": {\"output\": \"olleh\"},\n",
      " |                  \"event\": \"on_chain_end\",\n",
      " |                  \"metadata\": {},\n",
      " |                  \"name\": \"reverse\",\n",
      " |                  \"tags\": [],\n",
      " |              },\n",
      " |          ]\n",
      " |\n",
      " |\n",
      " |      Example: Dispatch Custom Event\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.callbacks.manager import (\n",
      " |              adispatch_custom_event,\n",
      " |          )\n",
      " |          from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
      " |          import asyncio\n",
      " |\n",
      " |\n",
      " |          async def slow_thing(some_input: str, config: RunnableConfig) -> str:\n",
      " |              \"\"\"Do something that takes a long time.\"\"\"\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              await adispatch_custom_event(\n",
      " |                  \"progress_event\",\n",
      " |                  {\"message\": \"Finished step 1 of 3\"},\n",
      " |                  config=config # Must be included for python < 3.10\n",
      " |              )\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              await adispatch_custom_event(\n",
      " |                  \"progress_event\",\n",
      " |                  {\"message\": \"Finished step 2 of 3\"},\n",
      " |                  config=config # Must be included for python < 3.10\n",
      " |              )\n",
      " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
      " |              return \"Done\"\n",
      " |\n",
      " |          slow_thing = RunnableLambda(slow_thing)\n",
      " |\n",
      " |          async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n",
      " |              print(event)\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable.\n",
      " |          version: The version of the schema to use either `v2` or `v1`.\n",
      " |                   Users should use `v2`.\n",
      " |                   `v1` is for backwards compatibility and will be deprecated\n",
      " |                   in 0.4.0.\n",
      " |                   No default will be assigned until the API is stabilized.\n",
      " |                   custom events will only be surfaced in `v2`.\n",
      " |          include_names: Only include events from runnables with matching names.\n",
      " |          include_types: Only include events from runnables with matching types.\n",
      " |          include_tags: Only include events from runnables with matching tags.\n",
      " |          exclude_names: Exclude events from runnables with matching names.\n",
      " |          exclude_types: Exclude events from runnables with matching types.\n",
      " |          exclude_tags: Exclude events from runnables with matching tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |              These will be passed to astream_log as this implementation\n",
      " |              of astream_events is built on top of astream_log.\n",
      " |\n",
      " |      Yields:\n",
      " |          An async stream of StreamEvents.\n",
      " |\n",
      " |      Raises:\n",
      " |          NotImplementedError: If the version is not `v1` or `v2`.\n",
      " |\n",
      " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
      " |      Stream all output from a Runnable, as reported to the callback system.\n",
      " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
      " |\n",
      " |      Output is streamed as Log objects, which include a list of\n",
      " |      Jsonpatch ops that describe how the state of the run has changed in each\n",
      " |      step, and the final state of the run.\n",
      " |\n",
      " |      The Jsonpatch ops can be applied in order to construct state.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable.\n",
      " |          diff: Whether to yield diffs between each step or the current state.\n",
      " |          with_streamed_output_list: Whether to yield the streamed_output list.\n",
      " |          include_names: Only include logs with these names.\n",
      " |          include_types: Only include logs with these types.\n",
      " |          include_tags: Only include logs with these tags.\n",
      " |          exclude_names: Exclude logs with these names.\n",
      " |          exclude_types: Exclude logs with these types.\n",
      " |          exclude_tags: Exclude logs with these tags.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          A RunLogPatch or RunLog object.\n",
      " |\n",
      " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
      " |      Default implementation of atransform, which buffers input and calls astream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |      Args:\n",
      " |          input: An async iterator of inputs to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |\n",
      " |  batch(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'List[Output]'\n",
      " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
      " |\n",
      " |      The default implementation of batch works well for IO bound runnables.\n",
      " |\n",
      " |      Subclasses should override this method if they can batch more efficiently;\n",
      " |      e.g., if the underlying Runnable uses an API which supports a batch mode.\n",
      " |\n",
      " |  batch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'Iterator[Tuple[int, Union[Output, Exception]]]'\n",
      " |      Run invoke in parallel on a list of inputs,\n",
      " |      yielding results as they complete.\n",
      " |\n",
      " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind arguments to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      Useful when a Runnable in a chain requires an argument that is not\n",
      " |      in the output of the previous Runnable or included in the user input.\n",
      " |\n",
      " |      Args:\n",
      " |          kwargs: The arguments to bind to the Runnable.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the arguments bound.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_community.chat_models import ChatOllama\n",
      " |          from langchain_core.output_parsers import StrOutputParser\n",
      " |\n",
      " |          llm = ChatOllama(model='llama2')\n",
      " |\n",
      " |          # Without bind.\n",
      " |          chain = (\n",
      " |              llm\n",
      " |              | StrOutputParser()\n",
      " |          )\n",
      " |\n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two three four five.'\n",
      " |\n",
      " |          # With bind.\n",
      " |          chain = (\n",
      " |              llm.bind(stop=[\"three\"])\n",
      " |              | StrOutputParser()\n",
      " |          )\n",
      " |\n",
      " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
      " |          # Output is 'One two'\n",
      " |\n",
      " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'Type[BaseModel]'\n",
      " |      The type of config this Runnable accepts specified as a pydantic model.\n",
      " |\n",
      " |      To mark a field as configurable, see the `configurable_fields`\n",
      " |      and `configurable_alternatives` methods.\n",
      " |\n",
      " |      Args:\n",
      " |          include: A list of fields to include in the config schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate config.\n",
      " |\n",
      " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
      " |      Return a graph representation of this Runnable.\n",
      " |\n",
      " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate input to the Runnable.\n",
      " |\n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic input schema that depends on which\n",
      " |      configuration the Runnable is invoked with.\n",
      " |\n",
      " |      This method allows to get an input schema for a specific configuration.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate input.\n",
      " |\n",
      " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
      " |      Get the name of the Runnable.\n",
      " |\n",
      " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'Type[BaseModel]'\n",
      " |      Get a pydantic model that can be used to validate output to the Runnable.\n",
      " |\n",
      " |      Runnables that leverage the configurable_fields and configurable_alternatives\n",
      " |      methods will have a dynamic output schema that depends on which\n",
      " |      configuration the Runnable is invoked with.\n",
      " |\n",
      " |      This method allows to get an output schema for a specific configuration.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A config to use when generating the schema.\n",
      " |\n",
      " |      Returns:\n",
      " |          A pydantic model that can be used to validate output.\n",
      " |\n",
      " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'List[BasePromptTemplate]'\n",
      " |      Return a list of prompts used by this Runnable.\n",
      " |\n",
      " |  map(self) -> 'Runnable[List[Input], List[Output]]'\n",
      " |      Return a new Runnable that maps a list of inputs to a list of outputs,\n",
      " |      by calling invoke() with each input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that maps a list of inputs to a list of outputs.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |                  from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |                  def _lambda(x: int) -> int:\n",
      " |                      return x + 1\n",
      " |\n",
      " |                  runnable = RunnableLambda(_lambda)\n",
      " |                  print(runnable.map().invoke([1, 2, 3])) # [2, 3, 4]\n",
      " |\n",
      " |  pick(self, keys: 'Union[str, List[str]]') -> 'RunnableSerializable[Any, Any]'\n",
      " |      Pick keys from the dict output of this Runnable.\n",
      " |\n",
      " |      Pick single key:\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              import json\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |\n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              chain = RunnableMap(str=as_str, json=as_json)\n",
      " |\n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n",
      " |\n",
      " |              json_only_chain = chain.pick(\"json\")\n",
      " |              json_only_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> [1, 2, 3]\n",
      " |\n",
      " |      Pick list of keys:\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from typing import Any\n",
      " |\n",
      " |              import json\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
      " |\n",
      " |              as_str = RunnableLambda(str)\n",
      " |              as_json = RunnableLambda(json.loads)\n",
      " |              def as_bytes(x: Any) -> bytes:\n",
      " |                  return bytes(x, \"utf-8\")\n",
      " |\n",
      " |              chain = RunnableMap(\n",
      " |                  str=as_str,\n",
      " |                  json=as_json,\n",
      " |                  bytes=RunnableLambda(as_bytes)\n",
      " |              )\n",
      " |\n",
      " |              chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |\n",
      " |              json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n",
      " |              json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n",
      " |              # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
      " |\n",
      " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
      " |      Compose this Runnable with Runnable-like objects to make a RunnableSequence.\n",
      " |\n",
      " |      Equivalent to `RunnableSequence(self, *others)` or `self | others[0] | ...`\n",
      " |\n",
      " |      Example:\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |              def add_one(x: int) -> int:\n",
      " |                  return x + 1\n",
      " |\n",
      " |              def mul_two(x: int) -> int:\n",
      " |                  return x * 2\n",
      " |\n",
      " |              runnable_1 = RunnableLambda(add_one)\n",
      " |              runnable_2 = RunnableLambda(mul_two)\n",
      " |              sequence = runnable_1.pipe(runnable_2)\n",
      " |              # Or equivalently:\n",
      " |              # sequence = runnable_1 | runnable_2\n",
      " |              # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
      " |              sequence.invoke(1)\n",
      " |              await sequence.ainvoke(1)\n",
      " |              # -> 4\n",
      " |\n",
      " |              sequence.batch([1, 2, 3])\n",
      " |              await sequence.abatch([1, 2, 3])\n",
      " |              # -> [4, 6, 8]\n",
      " |\n",
      " |  stream(self, input: 'Input', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of stream, which calls invoke.\n",
      " |      Subclasses should override this method if they support streaming output.\n",
      " |\n",
      " |      Args:\n",
      " |          input: The input to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |\n",
      " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
      " |      Default implementation of transform, which buffers input and then calls stream.\n",
      " |      Subclasses should override this method if they can start producing output while\n",
      " |      input is still being generated.\n",
      " |\n",
      " |      Args:\n",
      " |          input: An iterator of inputs to the Runnable.\n",
      " |          config: The config to use for the Runnable. Defaults to None.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Yields:\n",
      " |          The output of the Runnable.\n",
      " |\n",
      " |  with_alisteners(self, *, on_start: 'Optional[AsyncListener]' = None, on_end: 'Optional[AsyncListener]' = None, on_error: 'Optional[AsyncListener]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind asynchronous lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      on_start: Asynchronously called before the Runnable starts running.\n",
      " |      on_end: Asynchronously called after the Runnable finishes running.\n",
      " |      on_error: Asynchronously called if the Runnable throws an error.\n",
      " |\n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |\n",
      " |      Args:\n",
      " |          on_start: Asynchronously called before the Runnable starts running.\n",
      " |              Defaults to None.\n",
      " |          on_end: Asynchronously called after the Runnable finishes running.\n",
      " |              Defaults to None.\n",
      " |          on_error: Asynchronously called if the Runnable throws an error.\n",
      " |              Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the listeners bound.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          import time\n",
      " |\n",
      " |          async def test_runnable(time_to_sleep : int):\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n",
      " |              await asyncio.sleep(time_to_sleep)\n",
      " |              print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n",
      " |\n",
      " |          async def fn_start(run_obj : Runnable):\n",
      " |              print(f\"on start callback starts at {format_t(time.time())}\n",
      " |              await asyncio.sleep(3)\n",
      " |              print(f\"on start callback ends at {format_t(time.time())}\")\n",
      " |\n",
      " |          async def fn_end(run_obj : Runnable):\n",
      " |              print(f\"on end callback starts at {format_t(time.time())}\n",
      " |              await asyncio.sleep(2)\n",
      " |              print(f\"on end callback ends at {format_t(time.time())}\")\n",
      " |\n",
      " |          runnable = RunnableLambda(test_runnable).with_alisteners(\n",
      " |              on_start=fn_start,\n",
      " |              on_end=fn_end\n",
      " |          )\n",
      " |          async def concurrent_runs():\n",
      " |              await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n",
      " |\n",
      " |          asyncio.run(concurrent_runs())\n",
      " |          Result:\n",
      " |          on start callback starts at 2024-05-16T14:20:29.637053+00:00\n",
      " |          on start callback starts at 2024-05-16T14:20:29.637150+00:00\n",
      " |          on start callback ends at 2024-05-16T14:20:32.638305+00:00\n",
      " |          on start callback ends at 2024-05-16T14:20:32.638383+00:00\n",
      " |          Runnable[3s]: starts at 2024-05-16T14:20:32.638849+00:00\n",
      " |          Runnable[5s]: starts at 2024-05-16T14:20:32.638999+00:00\n",
      " |          Runnable[3s]: ends at 2024-05-16T14:20:35.640016+00:00\n",
      " |          on end callback starts at 2024-05-16T14:20:35.640534+00:00\n",
      " |          Runnable[5s]: ends at 2024-05-16T14:20:37.640169+00:00\n",
      " |          on end callback starts at 2024-05-16T14:20:37.640574+00:00\n",
      " |          on end callback ends at 2024-05-16T14:20:37.640654+00:00\n",
      " |          on end callback ends at 2024-05-16T14:20:39.641751+00:00\n",
      " |\n",
      " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
      " |      Bind config to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          config: The config to bind to the Runnable.\n",
      " |          kwargs: Additional keyword arguments to pass to the Runnable.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the config bound.\n",
      " |\n",
      " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
      " |      Add fallbacks to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      The new Runnable will try the original Runnable, and then each fallback\n",
      " |      in order, upon failures.\n",
      " |\n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original Runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |              Defaults to (Exception,).\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base Runnable\n",
      " |              and its fallbacks must accept a dictionary as input. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original Runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |          .. code-block:: python\n",
      " |\n",
      " |              from typing import Iterator\n",
      " |\n",
      " |              from langchain_core.runnables import RunnableGenerator\n",
      " |\n",
      " |\n",
      " |              def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
      " |                  raise ValueError()\n",
      " |                  yield \"\"\n",
      " |\n",
      " |\n",
      " |              def _generate(input: Iterator) -> Iterator[str]:\n",
      " |                  yield from \"foo bar\"\n",
      " |\n",
      " |\n",
      " |              runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
      " |                  [RunnableGenerator(_generate)]\n",
      " |                  )\n",
      " |              print(''.join(runnable.stream({}))) #foo bar\n",
      " |\n",
      " |      Args:\n",
      " |          fallbacks: A sequence of runnables to try if the original Runnable fails.\n",
      " |          exceptions_to_handle: A tuple of exception types to handle.\n",
      " |          exception_key: If string is specified then handled exceptions will be passed\n",
      " |              to fallbacks as part of the input under the specified key. If None,\n",
      " |              exceptions will not be passed to fallbacks. If used, the base Runnable\n",
      " |              and its fallbacks must accept a dictionary as input.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that will try the original Runnable, and then each\n",
      " |          fallback in order, upon failures.\n",
      " |\n",
      " |  with_listeners(self, *, on_start: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_end: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_error: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind lifecycle listeners to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      on_start: Called before the Runnable starts running, with the Run object.\n",
      " |      on_end: Called after the Runnable finishes running, with the Run object.\n",
      " |      on_error: Called if the Runnable throws an error, with the Run object.\n",
      " |\n",
      " |      The Run object contains information about the run, including its id,\n",
      " |      type, input, output, error, start_time, end_time, and any tags or metadata\n",
      " |      added to the run.\n",
      " |\n",
      " |      Args:\n",
      " |          on_start: Called before the Runnable starts running. Defaults to None.\n",
      " |          on_end: Called after the Runnable finishes running. Defaults to None.\n",
      " |          on_error: Called if the Runnable throws an error. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the listeners bound.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |          from langchain_core.tracers.schemas import Run\n",
      " |\n",
      " |          import time\n",
      " |\n",
      " |          def test_runnable(time_to_sleep : int):\n",
      " |              time.sleep(time_to_sleep)\n",
      " |\n",
      " |          def fn_start(run_obj: Run):\n",
      " |              print(\"start_time:\", run_obj.start_time)\n",
      " |\n",
      " |          def fn_end(run_obj: Run):\n",
      " |              print(\"end_time:\", run_obj.end_time)\n",
      " |\n",
      " |          chain = RunnableLambda(test_runnable).with_listeners(\n",
      " |              on_start=fn_start,\n",
      " |              on_end=fn_end\n",
      " |          )\n",
      " |          chain.invoke(2)\n",
      " |\n",
      " |  with_retry(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
      " |      Create a new Runnable that retries the original Runnable on exceptions.\n",
      " |\n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on.\n",
      " |              Defaults to (Exception,).\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait\n",
      " |              time between retries. Defaults to True.\n",
      " |          stop_after_attempt: The maximum number of attempts to make before\n",
      " |              giving up. Defaults to 3.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original Runnable on exceptions.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      .. code-block:: python\n",
      " |\n",
      " |          from langchain_core.runnables import RunnableLambda\n",
      " |\n",
      " |          count = 0\n",
      " |\n",
      " |\n",
      " |          def _lambda(x: int) -> None:\n",
      " |              global count\n",
      " |              count = count + 1\n",
      " |              if x == 1:\n",
      " |                  raise ValueError(\"x is 1\")\n",
      " |              else:\n",
      " |                   pass\n",
      " |\n",
      " |\n",
      " |          runnable = RunnableLambda(_lambda)\n",
      " |          try:\n",
      " |              runnable.with_retry(\n",
      " |                  stop_after_attempt=2,\n",
      " |                  retry_if_exception_type=(ValueError,),\n",
      " |              ).invoke(1)\n",
      " |          except ValueError:\n",
      " |              pass\n",
      " |\n",
      " |          assert (count == 2)\n",
      " |\n",
      " |\n",
      " |      Args:\n",
      " |          retry_if_exception_type: A tuple of exception types to retry on\n",
      " |          wait_exponential_jitter: Whether to add jitter to the wait time\n",
      " |                                   between retries\n",
      " |          stop_after_attempt: The maximum number of attempts to make before giving up\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable that retries the original Runnable on exceptions.\n",
      " |\n",
      " |  with_types(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
      " |      Bind input and output types to a Runnable, returning a new Runnable.\n",
      " |\n",
      " |      Args:\n",
      " |          input_type: The input type to bind to the Runnable. Defaults to None.\n",
      " |          output_type: The output type to bind to the Runnable. Defaults to None.\n",
      " |\n",
      " |      Returns:\n",
      " |          A new Runnable with the types bound.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  InputType\n",
      " |      The type of input this Runnable accepts specified as a type annotation.\n",
      " |\n",
      " |  OutputType\n",
      " |      The type of output this Runnable produces specified as a type annotation.\n",
      " |\n",
      " |  config_specs\n",
      " |      List configurable fields for this Runnable.\n",
      " |\n",
      " |  input_schema\n",
      " |      The type of input this Runnable accepts specified as a pydantic model.\n",
      " |\n",
      " |  output_schema\n",
      " |      The type of output this Runnable produces specified as a pydantic model.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from langchain_core.runnables.base.Runnable:\n",
      " |\n",
      " |  __orig_bases__ = (typing.Generic[-Input, +Output], <class 'abc.ABC'>)\n",
      " |\n",
      " |  name = None\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |\n",
      " |  __class_getitem__(...)\n",
      " |      Parameterizes a generic class.\n",
      " |\n",
      " |      At least, parameterizing a generic class is the *main* thing this\n",
      " |      method does. For example, for some generic class `Foo`, this is called\n",
      " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      " |\n",
      " |      However, note that this method is also called when defining generic\n",
      " |      classes in the first place with `class Foo[T]: ...`.\n",
      " |\n",
      " |  __init_subclass__(...)\n",
      " |      Function to initialize subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ToolNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9617b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi there! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\": (\"user\", \"Hello\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b608c5-0c15-4fb7-aa24-80ce5774fb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Multiply 3 and 2\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  multiply (call_WYT9qhOnwPlRwCy8pld51pwC)\n",
      " Call ID: call_WYT9qhOnwPlRwCy8pld51pwC\n",
      "  Args:\n",
      "    a: 3\n",
      "    b: 2\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: multiply\n",
      "\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "messages = graph.invoke({\"messages\": (\"user\", \"Multiply 3 and 2\")})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34708377-16b6-4474-9e23-71890c1fb36e",
   "metadata": {},
   "source": [
    "Now, we can see that the graph runs the tool!\n",
    "\n",
    "It responds with a `ToolMessage`. \n",
    "\n",
    "## LangGraph Studio\n",
    "\n",
    "--\n",
    "\n",
    "**⚠️ DISCLAIMER**\n",
    "\n",
    "*Running Studio currently requires a Mac. If you are not using a Mac, then skip this step.*\n",
    "\n",
    "--\n",
    "\n",
    "Load the `router` in Studio, which uses `module-1/studio/router.py` set in `module-1/studio/langgraph.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43782c33-0f41-47f2-ae38-ddb2cd4ba6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
